{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def trim_start_end_nans(df):\n",
    "    \"\"\"\n",
    "    Removes rows at the start and end of a DataFrame that have NaN values in any column.\n",
    "    \"\"\"\n",
    "    # Initialize start_idx and end_idx based on the DataFrame's index type\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        start_idx = df.index[0]  # Assume first index is earliest; adjust if necessary\n",
    "        end_idx = df.index[-1]  # Assume last index is latest; adjust if necessary\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = len(df) - 1\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Find the first non-NaN index in the current column\n",
    "        first_valid_index = df[column].first_valid_index()\n",
    "        if first_valid_index is not None and df.index.get_loc(\n",
    "            first_valid_index\n",
    "        ) > df.index.get_loc(start_idx):\n",
    "            start_idx = first_valid_index\n",
    "\n",
    "        # Find the last non-NaN index in the current column\n",
    "        last_valid_index = df[column].last_valid_index()\n",
    "        if last_valid_index is not None and df.index.get_loc(\n",
    "            last_valid_index\n",
    "        ) < df.index.get_loc(end_idx):\n",
    "            end_idx = last_valid_index\n",
    "\n",
    "    # Trim the DataFrame\n",
    "    return df.loc[start_idx:end_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_data_for_plot(\n",
    "    plot_number,\n",
    "    target_columns,\n",
    "    continuous_columns,\n",
    "    start_date=\"2023-07-20\",\n",
    "    end_date=\"2023-09-03\",\n",
    "    rolling_windows=[3, 7], \n",
    "):\n",
    "    \"\"\"\n",
    "    Process data for a given plot number within a specified date range. This includes:\n",
    "    * Spike Detection (up and down) for VWC columns\n",
    "    * Time since last significant precipitation\n",
    "    * Cumulative precipitation within a time window\n",
    "    * Rolling window statistics\n",
    "    * Time Encoding\n",
    "    \"\"\"\n",
    "\n",
    "    # Database connection\n",
    "    conn = sqlite3.connect(\"processed_data.db\")\n",
    "    query = \"SELECT * FROM data_table\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Filter by plot_number and date range\n",
    "    df = df[\n",
    "        (df[\"plot_number\"] == plot_number)\n",
    "        & (df[\"TIMESTAMP\"] >= start_date)\n",
    "        & (df[\"TIMESTAMP\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Convert TIMESTAMP to datetime\n",
    "    df[\"TIMESTAMP\"] = pd.to_datetime(df[\"TIMESTAMP\"])\n",
    "    df.set_index(\"TIMESTAMP\", inplace=True)\n",
    "\n",
    "    # Sort by TIMESTAMP \n",
    "    df.sort_values(by=\"TIMESTAMP\", inplace=True)\n",
    "\n",
    "    # Select relevant columns\n",
    "    df = df[continuous_columns + target_columns]\n",
    "\n",
    "    # Resample to daily frequency \n",
    "    df = df.resample(\"D\").mean()\n",
    "\n",
    "    # Spike detection for VWC columns\n",
    "    for col in df.columns:\n",
    "        if \"VWC\" in col:\n",
    "            df[f\"{col}_spike_up\"] = (df[col] > df[col].shift(1) * 1.15).astype(int)  # 15% increase\n",
    "            df[f\"{col}_spike_down\"] = (df[col] < df[col].shift(1) * 0.85).astype(int)  # 15% decrease\n",
    "\n",
    "    # Time features\n",
    "    df['time_index'] = np.arange(len(df))\n",
    "\n",
    "    # Time since precipitation (modify thresholds as needed)\n",
    "    significant_precip_threshold = 0.5  \n",
    "    max_precip_value = df['precip_irrig'].max()\n",
    "    df['time_since_last_significant_precip'] = (df['precip_irrig'] > significant_precip_threshold).astype(int)\n",
    "    df['time_since_last_significant_precip'] = df['time_since_last_significant_precip'].replace(to_replace=0, method='ffill')\n",
    "    df['time_since_last_half_max_precip'] = (df['precip_irrig'] > (max_precip_value / 2)).astype(int)\n",
    "    df['time_since_last_half_max_precip'] = df['time_since_last_half_max_precip'].replace(to_replace=0, method='ffill')\n",
    "\n",
    "    # Cumulative precipitation (replace 4 with the desired window)\n",
    "    df['precip_irrig_cumulative_4day'] = df['precip_irrig'].rolling(4).sum() \n",
    "\n",
    "    # Preprocessing \n",
    "    df = df.interpolate(method=\"pchip\")\n",
    "\n",
    "    # Rolling window features\n",
    "    for window in rolling_windows:\n",
    "        for col in continuous_columns:\n",
    "            df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "            df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window).std()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(df, target_columns, continuous_columns):\n",
    "    # Subtract mean from each column (append new columns with suffix \"_mean_subtracted\")\n",
    "    df_mean_subtracted = df.copy()\n",
    "    mean_values = {}\n",
    "    for col in df_mean_subtracted.columns:\n",
    "        if col in [target_columns + continuous_columns]:\n",
    "            mean_values[col] = df_mean_subtracted[col].mean()\n",
    "            df_mean_subtracted[col] = df_mean_subtracted[col] - mean_values[col]\n",
    "    return df_mean_subtracted, mean_values\n",
    "\n",
    "def create_derivative_columns(df, target_columns, continuous_columns):\n",
    "    initial_values = {}\n",
    "    for col in df.columns:  # Change to apply to all columns\n",
    "        if col in [target_columns + continuous_columns]:\n",
    "            initial_values[col] = df[col].iloc[0]\n",
    "        deriv_col_name = f\"{col}_deriv\" \n",
    "        df[deriv_col_name] = df[col].diff().fillna(0)  # Fill NaN with 0 for initial diff\n",
    "    return df, initial_values\n",
    "\n",
    "\n",
    "def transform_and_scale_data(df, target_columns, continuous_columns):\n",
    "    df_transformed = df.copy()\n",
    "    df_transformed, mean_values = subtract_mean(df_transformed, target_columns, continuous_columns)  # Change here to apply to all\n",
    "    df_transformed, initial_values = create_derivative_columns(df_transformed, target_columns, continuous_columns)\n",
    "    df_transformed[\"precip_irrig_bool\"] = df_transformed[\"precip_irrig\"].apply(\n",
    "        lambda x: 1 if x > 0 else 0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def train_and_save_model_with_time_series_validation(X, y, forecast_day, model_save_path):\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Best model placeholder\n",
    "    best_model = None\n",
    "    best_rmse = float(\"inf\")\n",
    "\n",
    "    # Iterate over each train-test split\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        # Split data\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Prepare DMatrices\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        # XGBoost parameters\n",
    "        param = {\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.05, \n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 1, \n",
    "        'lambda': 1,\n",
    "        'alpha': 0.2,\n",
    "        'gamma': 0.2 \n",
    "    }\n",
    "\n",
    "\n",
    "        num_round = 20  # Number of training iterations\n",
    "\n",
    "        # Train XGBoost model with early stopping\n",
    "        bst = xgb.train(\n",
    "            param,\n",
    "            dtrain,\n",
    "            num_round,\n",
    "            [(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=True\n",
    "        )\n",
    "\n",
    "        # Predict on validation set\n",
    "        y_val_pred = bst.predict(dval)\n",
    "\n",
    "        # Calculate RMSE for the current split\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        print(f\"Validation RMSE for current split: {val_rmse:.2f}\")\n",
    "\n",
    "        # Update best model if improvement\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_model = bst\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_save_path = os.path.join(model_save_path, f\"best_model_day_{forecast_day}.pkl\")\n",
    "    os.makedirs(os.path.dirname(best_model_save_path), exist_ok=True)\n",
    "    with open(best_model_save_path, \"wb\") as model_file:\n",
    "        print(f\"***Saving best model to {best_model_save_path}\")\n",
    "        pickle.dump(best_model, model_file)\n",
    "\n",
    "    return best_model_save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the necessary functions are defined as provided: \n",
    "# trim_start_end_nans, process_data_for_plot, subtract_mean, create_derivative_columns, transform_and_scale_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_and_train_models(plot_numbers, target_column, continuous_columns, forecast_horizon, model_save_path):\n",
    "    \"\"\"\n",
    "    Prepare data, train models for each forecast horizon, and save the models.\n",
    "    \"\"\"\n",
    "    for plot_number in plot_numbers:\n",
    "        # Process and transform data\n",
    "        df = process_data_for_plot(plot_number, [target_column], continuous_columns)\n",
    "        df = trim_start_end_nans(df)\n",
    "        df_transformed = transform_and_scale_data(df, [target_column], continuous_columns)\n",
    "\n",
    "        # Define training data\n",
    "        X = df_transformed.drop(columns=[target_column]).values\n",
    "        y = df_transformed[target_column].values\n",
    "\n",
    "        # Train and save model for each forecast horizon\n",
    "        for forecast_day in range(1, forecast_horizon + 1):  # Example: 3-day forecast horizon\n",
    "            train_and_save_model_with_time_series_validation(X, y, forecast_day, model_save_path)\n",
    "            print(f\"Model for day {forecast_day} trained and saved for plot {plot_number}\")\n",
    "\n",
    "def predict_with_model(model_path, X):\n",
    "    \"\"\"\n",
    "    Load a model from a file and make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the model_path is correct and the file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    dtest = xgb.DMatrix(X)  \n",
    "    return model.predict(dtest)\n",
    "\n",
    "def run_inference(models_path, plot_number, target_column, continuous_columns, forecast_horizon):\n",
    "    # Process and transform data for inference\n",
    "    df = process_data_for_plot(plot_number, [target_column], continuous_columns)\n",
    "    df = trim_start_end_nans(df)\n",
    "    df_transformed = transform_and_scale_data(df, [target_column], continuous_columns)\n",
    "\n",
    "    X = df_transformed.drop(columns=[target_column]).values\n",
    "    y_actual = df_transformed[target_column].values\n",
    "\n",
    "    predictions = np.zeros((len(X) - forecast_horizon + 1, forecast_horizon))\n",
    "\n",
    "    for day in range(1, forecast_horizon + 1):\n",
    "        models_path = os.path.join(models_path, f\"best_model_day_{day}.pkl\")\n",
    "        pred = predict_with_model(models_path, X[:-(forecast_horizon - day) if (forecast_horizon - day) > 0 else None])\n",
    "        for i in range(min(len(pred), len(predictions))):\n",
    "            predictions[i, day - 1] = pred[i]\n",
    "\n",
    "    # Filter out rows where all values are zero (assuming zero predictions are not expected)\n",
    "    predictions = predictions[~np.all(predictions == 0, axis=1)]\n",
    "\n",
    "    # Flatten predictions and actuals to plot on the same curve\n",
    "    flat_predictions = predictions.flatten()\n",
    "    # Adjust actuals to match the length of filtered predictions\n",
    "    adjusted_length = len(flat_predictions) // forecast_horizon\n",
    "    flat_actuals = np.array([y_actual[i:i+forecast_horizon] for i in range(adjusted_length)]).flatten()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(flat_actuals, label='Actual', linestyle='-', marker='o')\n",
    "    plt.plot(flat_predictions, label='Predicted', linestyle='--', marker='x')\n",
    "\n",
    "    plt.title(f'Predictions vs Actual Values for Plot {plot_number}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel(target_column)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "plot_numbers = [2001]  # Example plot numbers for training\n",
    "target_column = \"VWC_06\"\n",
    "continuous_columns = [\n",
    "    \"Ta_2m_Avg\", \"RH_2m_Avg\", \"Solar_2m_Avg\", \"WndAveSpd_3m\", \"Rain_1m_Tot\",\n",
    "    \"Dp_2m_Avg\", \"TaMax_2m\", \"TaMin_2m\", \"RHMax_2m\", \"RHMin_2m\",\n",
    "    \"HeatIndex_2m_Avg\", \"irrigation\", \"precip_irrig\", \"canopy_temp\",\n",
    "    \"VWC_18\", \"VWC_30\"\n",
    "]\n",
    "\n",
    "model_save_path = os.path.join(os.getcwd(), \"models\") \n",
    "\n",
    "print(model_save_path)\n",
    "forecast_horizon = 4\n",
    "\n",
    "# Train models\n",
    "prepare_and_train_models(plot_numbers, target_column, continuous_columns, forecast_horizon, model_save_path)\n",
    "\n",
    "# Inference\n",
    "plot_number_for_inference = 2007  # Example plot number for inference\n",
    "predictions = run_inference(model_save_path, plot_number_for_inference, target_column, continuous_columns, forecast_horizon)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "plot_number_for_inference = 2007  # Example plot number for inference\n",
    "predictions = run_inference(model_save_path, plot_number_for_inference, target_column, continuous_columns, forecast_horizon)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
