{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 1: Data Preprocessing\n",
    "\n",
    "In this step, we prepare the data for interpolation. We will identify the missing values (NaNs) and set up the Kalman Filter parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0             30\n",
      "TIMESTAMP               0\n",
      "Ta_2m_Avg              30\n",
      "RH_2m_Avg              30\n",
      "Solar_2m_Avg           30\n",
      "WndAveSpd_3m           30\n",
      "Rain_1m_Tot            30\n",
      "Dp_2m_Avg              30\n",
      "TaMax_2m               30\n",
      "TaMin_2m               30\n",
      "RHMax_2m               30\n",
      "RHMin_2m               30\n",
      "HeatIndex_2m_Avg       30\n",
      "Elevation              30\n",
      "canopy_temp         22440\n",
      "crop                   30\n",
      "growth_stage         5556\n",
      "VWC_06              23218\n",
      "VWC_18              23043\n",
      "VWC_30              21713\n",
      "VWC_42              32790\n",
      "plot_number             0\n",
      "daily_et               30\n",
      "CWSI                21990\n",
      "SWSI                28229\n",
      "irrigation          37314\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Segment 1: Initial Setup\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3 as sql\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "# Load dataset\n",
    "file_path = './processed_data.db'  # Adjust path as necessary\n",
    "with sql.connect(file_path) as con:\n",
    "    df = pd.read_sql_query('SELECT * from data_table', con)\n",
    "\n",
    "# Define columns to interpolate\n",
    "columns_to_interpolate = ['canopy_temp', 'VWC_06', 'VWC_18', 'VWC_30', 'VWC_42']\n",
    "\n",
    "#count numer of rows with nans in the data\n",
    "print(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 2: Kalman Filtering Interpolation\n",
    "\n",
    "We create a function to apply the Kalman Filter for interpolation. The filter will be applied only to the missing values in each time series, and existing values will remain unchanged. The predictions will be single floating-point numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bnsoh2\\AppData\\Local\\Temp\\ipykernel_320\\352143969.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_interpolated = df.groupby('plot_number', group_keys=False).apply(interpolate_group)\n"
     ]
    }
   ],
   "source": [
    "# Function to apply spline interpolation on a sorted group\n",
    "def interpolate_group(group):\n",
    "    # Ensure the group is sorted by its index (TIMESTAMP) in ascending order\n",
    "    group_sorted = group.sort_index()\n",
    "    \n",
    "    # Apply spline interpolation to each specified column within the sorted group\n",
    "    for column in columns_to_interpolate:\n",
    "        # Check if the column exists in the group to avoid KeyError\n",
    "        if column in group_sorted.columns:\n",
    "            group_sorted[column] = group_sorted[column].interpolate(method='spline', order=2, limit_direction='both')\n",
    "    \n",
    "    return group_sorted\n",
    "\n",
    "# Group the DataFrame by 'plot_number' and apply the interpolation function to each group\n",
    "df_interpolated = df.groupby('plot_number', group_keys=False).apply(interpolate_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your original DataFrame with 'TIMESTAMP' and 'plot_number' columns\n",
    "# Convert 'TIMESTAMP' to a datetime format if it's not already\n",
    "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "\n",
    "# Convert 'TIMESTAMP' to a numerical value\n",
    "# This will convert the timestamp to the number of seconds since the minimum timestamp in your dataset\n",
    "df['TIMESTAMP_NUMERIC'] = (df['TIMESTAMP'] - df['TIMESTAMP'].min()).dt.total_seconds()\n",
    "\n",
    "# Now proceed with your interpolation as before\n",
    "\n",
    "\n",
    "for plot in df['plot_number'].unique():\n",
    "    # Filter data for the current plot number\n",
    "    plot_df = df[df['plot_number'] == plot].copy()\n",
    "\n",
    "    # For each column that requires interpolation, fit a polynomial and fill in the gaps\n",
    "    for column in columns_to_interpolate:\n",
    "        # Extract the current series\n",
    "        series = plot_df[column]\n",
    "\n",
    "        # Prepare the data, removing NaNs for fitting the polynomial\n",
    "        x = plot_df['TIMESTAMP_NUMERIC']\n",
    "        y = series\n",
    "        valid_mask = ~y.isna()\n",
    "        x_valid = x[valid_mask]\n",
    "        y_valid = y[valid_mask]\n",
    "\n",
    "        # Check if we have enough data points to fit the specified degree of the polynomial\n",
    "        if len(x_valid) > 5:  # More than degree of polynomial\n",
    "            # Fit the polynomial (degree 5 is used here, but you might need to adjust this)\n",
    "            coefficients = np.polyfit(x_valid, y_valid, 5)\n",
    "\n",
    "            # Create a polynomial function from the coefficients\n",
    "            polynomial = np.poly1d(coefficients)\n",
    "\n",
    "            # Use this polynomial to interpolate the missing values\n",
    "            interpolated_values = polynomial(x)\n",
    "\n",
    "            # Fill in the missing values in the original data\n",
    "            plot_df[column] = series.where(series.notna(), interpolated_values)\n",
    "\n",
    "    # Append the interpolated plot data to the aggregated DataFrame\n",
    "    df_interpolated = pd.concat([df_interpolated, plot_df])\n",
    "\n",
    "# Dropping the auxiliary numeric timestamp column\n",
    "df_interpolated.drop('TIMESTAMP_NUMERIC', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated data saved to kalman_interpolated_data.db\n"
     ]
    }
   ],
   "source": [
    "# Segment 4: Output\n",
    "\n",
    "# Save the interpolated data into a new CSV file\n",
    "interpolated_file_path = 'kalman_interpolated_data.db'\n",
    "\n",
    "# create the database and save the data\n",
    "with sql.connect(interpolated_file_path) as con:\n",
    "    df_interpolated.to_sql('kalman_interpolated_data', con, index=False, if_exists='replace')\n",
    "\n",
    "\n",
    "# Confirm the save\n",
    "print(f\"Interpolated data saved to {interpolated_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolating Across All Plot Numbers\n",
    "\n",
    "Now, we'll extend the interpolation to all plot numbers in the dataset. We'll loop through each unique plot number, apply the interpolation, and combine the results into one DataFrame.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
