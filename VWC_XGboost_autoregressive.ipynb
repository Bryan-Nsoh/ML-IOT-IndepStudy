{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def trim_start_end_nans(df):\n",
    "    \"\"\"\n",
    "    Removes rows at the start and end of a DataFrame that have NaN values in any column.\n",
    "    \"\"\"\n",
    "    # Initialize start_idx and end_idx based on the DataFrame's index type\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        start_idx = df.index[0]  # Assume first index is earliest; adjust if necessary\n",
    "        end_idx = df.index[-1]  # Assume last index is latest; adjust if necessary\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = len(df) - 1\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Find the first non-NaN index in the current column\n",
    "        first_valid_index = df[column].first_valid_index()\n",
    "        if first_valid_index is not None and df.index.get_loc(\n",
    "            first_valid_index\n",
    "        ) > df.index.get_loc(start_idx):\n",
    "            start_idx = first_valid_index\n",
    "\n",
    "        # Find the last non-NaN index in the current column\n",
    "        last_valid_index = df[column].last_valid_index()\n",
    "        if last_valid_index is not None and df.index.get_loc(\n",
    "            last_valid_index\n",
    "        ) < df.index.get_loc(end_idx):\n",
    "            end_idx = last_valid_index\n",
    "\n",
    "    # Trim the DataFrame\n",
    "    return df.loc[start_idx:end_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_data_for_plot(\n",
    "    plot_number,\n",
    "    target_columns,\n",
    "    continuous_columns,\n",
    "    start_date=\"2023-07-20\",\n",
    "    end_date=\"2023-09-03\",\n",
    "    rolling_windows=[3, 7], \n",
    "):\n",
    "    \"\"\"\n",
    "    Process data for a given plot number within a specified date range. This includes:\n",
    "    * Spike Detection (up and down) for VWC columns\n",
    "    * Time since last significant precipitation\n",
    "    * Cumulative precipitation within a time window\n",
    "    * Rolling window statistics\n",
    "    * Time Encoding\n",
    "    \"\"\"\n",
    "\n",
    "    # Database connection\n",
    "    conn = sqlite3.connect(\"processed_data.db\")\n",
    "    query = \"SELECT * FROM data_table\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Filter by plot_number and date range\n",
    "    df = df[\n",
    "        (df[\"plot_number\"] == plot_number)\n",
    "        & (df[\"TIMESTAMP\"] >= start_date)\n",
    "        & (df[\"TIMESTAMP\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Convert TIMESTAMP to datetime\n",
    "    df[\"TIMESTAMP\"] = pd.to_datetime(df[\"TIMESTAMP\"])\n",
    "    df.set_index(\"TIMESTAMP\", inplace=True)\n",
    "\n",
    "    # Sort by TIMESTAMP \n",
    "    df.sort_values(by=\"TIMESTAMP\", inplace=True)\n",
    "\n",
    "    # Select relevant columns\n",
    "    df = df[continuous_columns + target_columns]\n",
    "\n",
    "    # Resample to daily frequency \n",
    "    df = df.resample(\"D\").mean()\n",
    "\n",
    "    # Spike detection for VWC columns\n",
    "    for col in df.columns:\n",
    "        if \"VWC\" in col:\n",
    "            df[f\"{col}_spike_up\"] = (df[col] > df[col].shift(1) * 1.15).astype(int)  # 15% increase\n",
    "            df[f\"{col}_spike_down\"] = (df[col] < df[col].shift(1) * 0.85).astype(int)  # 15% decrease\n",
    "\n",
    "    # Time features\n",
    "    df['time_index'] = np.arange(len(df))\n",
    "\n",
    "    # Time since precipitation (modify thresholds as needed)\n",
    "    significant_precip_threshold = 0.5  \n",
    "    max_precip_value = df['precip_irrig'].max()\n",
    "    df['time_since_last_significant_precip'] = (df['precip_irrig'] > significant_precip_threshold).astype(int)\n",
    "    df['time_since_last_significant_precip'] = df['time_since_last_significant_precip'].replace(to_replace=0, value=np.nan).fillna(method='ffill')\n",
    "    df['time_since_last_half_max_precip'] = (df['precip_irrig'] > (max_precip_value / 2)).astype(int)\n",
    "    df['time_since_last_half_max_precip'] = df['time_since_last_half_max_precip'].replace(to_replace=0, value=np.nan).fillna(method='ffill')\n",
    "\n",
    "\n",
    "    # Cumulative precipitation (replace 4 with the desired window)\n",
    "    df['precip_irrig_cumulative_4day'] = df['precip_irrig'].rolling(4).sum() \n",
    "\n",
    "    # Preprocessing \n",
    "    df = df.interpolate(method=\"pchip\")\n",
    "\n",
    "    # Rolling window features\n",
    "    for window in rolling_windows:\n",
    "        for col in continuous_columns:\n",
    "            df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "            df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window).std()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(df, target_columns, continuous_columns):\n",
    "    # Subtract mean from each column (append new columns with suffix \"_mean_subtracted\")\n",
    "    df_mean_subtracted = df.copy()\n",
    "    mean_values = {}\n",
    "    for col in df_mean_subtracted.columns:\n",
    "        if col in [target_columns + continuous_columns]:\n",
    "            mean_values[col] = df_mean_subtracted[col].mean()\n",
    "            df_mean_subtracted[col] = df_mean_subtracted[col] - mean_values[col]\n",
    "    return df_mean_subtracted, mean_values\n",
    "\n",
    "def create_derivative_columns(df, target_columns, continuous_columns):\n",
    "    initial_values = {}\n",
    "    for col in df.columns:  # Change to apply to all columns\n",
    "        if col in [target_columns + continuous_columns]:\n",
    "            initial_values[col] = df[col].iloc[0]\n",
    "        deriv_col_name = f\"{col}_deriv\" \n",
    "        df[deriv_col_name] = df[col].diff().fillna(0)  # Fill NaN with 0 for initial diff\n",
    "    return df, initial_values\n",
    "\n",
    "\n",
    "def transform_and_scale_data(df, target_columns, continuous_columns):\n",
    "    df_transformed = df.copy()\n",
    "    df_transformed, mean_values = subtract_mean(df_transformed, target_columns, continuous_columns)  # Change here to apply to all\n",
    "    df_transformed, initial_values = create_derivative_columns(df_transformed, target_columns, continuous_columns)\n",
    "    df_transformed[\"precip_irrig_bool\"] = df_transformed[\"precip_irrig\"].apply(\n",
    "        lambda x: 1 if x > 0 else 0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def generate_validation_targets(df, target_column, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Generate future target variables for validation, named as targetvar1, targetvar2, etc.\n",
    "    \"\"\"\n",
    "    for day in range(1, forecast_horizon + 1):\n",
    "        df[f\"{target_column}_{day}\"] = df[target_column].shift(-day)\n",
    "    return df\n",
    "\n",
    "def add_validation_targets(df, target_column, forecast_day):\n",
    "    \"\"\"\n",
    "    Adjust dataset to include validation targets and exclude them from training features.\n",
    "    \"\"\"\n",
    "    # Exclude the last 'forecast_day' rows to match the shifted targets\n",
    "    df = df.iloc[:-forecast_day] if forecast_day > 0 else df\n",
    "    return df\n",
    "\n",
    "def train_and_save_model_with_time_series_validation(X, y, validation_target, forecast_day, model_save_path):\n",
    "    \"\"\"\n",
    "    Train a model with time series cross-validation and save the best model.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    best_model, best_rmse = None, float(\"inf\")\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train = y[train_index]\n",
    "        y_val = validation_target[val_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        param = {'max_depth': 5, 'eta': 0.05, 'objective': 'reg:squarederror', 'eval_metric': 'rmse',\n",
    "                 'subsample': 0.8, 'colsample_bytree': 1, 'lambda': 1, 'alpha': 0.2, 'gamma': 0.2}\n",
    "        num_round = 2000\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=param,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=num_round,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=200,\n",
    "            verbose_eval=True\n",
    "        )\n",
    "\n",
    "\n",
    "        y_val_pred = bst.predict(dval)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse, best_model = val_rmse, bst\n",
    "\n",
    "    best_model_save_path = os.path.join(model_save_path, f\"best_model_day_{forecast_day}.pkl\")\n",
    "    os.makedirs(os.path.dirname(best_model_save_path), exist_ok=True)\n",
    "\n",
    "    with open(best_model_save_path, 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "\n",
    "    return best_model_save_path\n",
    "\n",
    "def prepare_and_train_models(plot_numbers, target_column, continuous_columns, forecast_horizon, model_save_path):\n",
    "    \"\"\"\n",
    "    Prepare data, train models for each forecast horizon, and save the models.\n",
    "    \"\"\"\n",
    "    for plot_number in plot_numbers:\n",
    "        # Placeholder for data preparation functions\n",
    "        df = process_data_for_plot(plot_number, [target_column], continuous_columns)\n",
    "        df = trim_start_end_nans(df)\n",
    "        df_transformed = transform_and_scale_data(df, [target_column], continuous_columns)\n",
    "        \n",
    "        # Generate validation targets\n",
    "        df_with_targets = generate_validation_targets(df_transformed, target_column, forecast_horizon)\n",
    "\n",
    "        for forecast_day in range(1, forecast_horizon + 1):\n",
    "            df_ready = add_validation_targets(df_with_targets, target_column, forecast_day)\n",
    "            \n",
    "            X = df_ready.drop(columns=[f\"{target_column}{i}\" for i in range(1, forecast_horizon + 1)]).values\n",
    "            y = df_ready[target_column].values\n",
    "            validation_target = df_ready[f\"{target_column}{forecast_day}\"].values\n",
    "\n",
    "            model_path = train_and_save_model_with_time_series_validation(X, y, validation_target, forecast_day, model_save_path)\n",
    "            print(f\"Model for day {forecast_day} trained and saved for plot {plot_number} at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bnsoh2\\OneDrive - University of Nebraska-Lincoln\\Projects\\Students\\Bryan Nsoh\\Indep_study_NsohGuo_2024\\ML\\models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bnsoh2\\AppData\\Local\\Temp\\ipykernel_12848\\3811550070.py:90: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['time_since_last_significant_precip'] = df['time_since_last_significant_precip'].replace(to_replace=0, value=np.nan).fillna(method='ffill')\n",
      "C:\\Users\\bnsoh2\\AppData\\Local\\Temp\\ipykernel_12848\\3811550070.py:92: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['time_since_last_half_max_precip'] = df['time_since_last_half_max_precip'].replace(to_replace=0, value=np.nan).fillna(method='ffill')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['VWC_064'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m forecast_horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[43mprepare_and_train_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_numbers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m     85\u001b[0m plot_number_for_inference \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2015\u001b[39m  \u001b[38;5;66;03m# Example plot number for inference\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 83\u001b[0m, in \u001b[0;36mprepare_and_train_models\u001b[1;34m(plot_numbers, target_column, continuous_columns, forecast_horizon, model_save_path)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m forecast_day \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, forecast_horizon \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     81\u001b[0m     df_ready \u001b[38;5;241m=\u001b[39m add_validation_targets(df_with_targets, target_column, forecast_day)\n\u001b[1;32m---> 83\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mdf_ready\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_column\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     84\u001b[0m     y \u001b[38;5;241m=\u001b[39m df_ready[target_column]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     85\u001b[0m     validation_target \u001b[38;5;241m=\u001b[39m df_ready[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mforecast_day\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\bnsoh2\\Desktop\\venvs\\ml_env\\Lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bnsoh2\\Desktop\\venvs\\ml_env\\Lib\\site-packages\\pandas\\core\\generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\bnsoh2\\Desktop\\venvs\\ml_env\\Lib\\site-packages\\pandas\\core\\generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4822\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4824\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4825\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4827\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bnsoh2\\Desktop\\venvs\\ml_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7070\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['VWC_064'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the necessary functions are defined as provided: \n",
    "# trim_start_end_nans, process_data_for_plot, subtract_mean, create_derivative_columns, transform_and_scale_data\n",
    "\n",
    "def predict_with_model(model_path, X):\n",
    "    \"\"\"\n",
    "    Load a model from a file and make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(model_path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    dtest = xgb.DMatrix(X)\n",
    "    return model.predict(dtest)\n",
    "\n",
    "\n",
    "def run_inference(models_path, plot_number, target_column, continuous_columns, forecast_horizon):\n",
    "    # Process and transform data for inference\n",
    "    df = process_data_for_plot(plot_number, [target_column], continuous_columns)\n",
    "    df = trim_start_end_nans(df)\n",
    "    df_transformed = transform_and_scale_data(df, [target_column], continuous_columns)\n",
    "\n",
    "    X = df_transformed.drop(columns=[target_column]).values\n",
    "    y_actual = df_transformed[target_column].values\n",
    "\n",
    "    predictions = np.zeros((len(X) - forecast_horizon + 1, forecast_horizon))\n",
    "\n",
    "    for day in range(1, forecast_horizon + 1):\n",
    "        model_path = os.path.join(models_path, f\"best_model_day_{day}.pkl\")\n",
    "        print(f\"Models path for day {day}: {models_path}\")\n",
    "        pred = predict_with_model(model_path, X[:-(forecast_horizon - day) if (forecast_horizon - day) > 0 else None])\n",
    "        for i in range(min(len(pred), len(predictions))):\n",
    "            predictions[i, day - 1] = pred[i]\n",
    "\n",
    "    # Filter out rows where all values are zero (assuming zero predictions are not expected)\n",
    "    predictions = predictions[~np.all(predictions == 0, axis=1)]\n",
    "\n",
    "    # Flatten predictions and actuals to plot on the same curve\n",
    "    flat_predictions = predictions.flatten()\n",
    "    # Adjust actuals to match the length of filtered predictions\n",
    "    adjusted_length = len(flat_predictions) // forecast_horizon\n",
    "    flat_actuals = np.array([y_actual[i:i+forecast_horizon] for i in range(adjusted_length)]).flatten()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(flat_actuals, label='Actual', linestyle='-', marker='o')\n",
    "    plt.plot(flat_predictions, label='Predicted', linestyle='--', marker='x')\n",
    "\n",
    "    plt.title(f'Predictions vs Actual Values for Plot {plot_number}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel(target_column)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "plot_numbers = [2007]  # Example plot numbers for training\n",
    "target_column = \"VWC_06\"\n",
    "continuous_columns = [\n",
    "    \"Ta_2m_Avg\", \"RH_2m_Avg\", \"Solar_2m_Avg\", \"WndAveSpd_3m\", \"Rain_1m_Tot\",\n",
    "    \"Dp_2m_Avg\", \"TaMax_2m\", \"TaMin_2m\", \"RHMax_2m\", \"RHMin_2m\",\n",
    "    \"HeatIndex_2m_Avg\", \"irrigation\", \"precip_irrig\", \"canopy_temp\",\n",
    "    \"VWC_18\", \"VWC_30\"\n",
    "]\n",
    "\n",
    "model_save_path = os.path.join(os.getcwd(), \"models\") \n",
    "\n",
    "print(model_save_path)\n",
    "forecast_horizon = 4\n",
    "\n",
    "# Train models\n",
    "prepare_and_train_models(plot_numbers, target_column, continuous_columns, forecast_horizon, model_save_path)\n",
    "\n",
    "# Inference\n",
    "plot_number_for_inference = 2015  # Example plot number for inference\n",
    "predictions = run_inference(model_save_path, plot_number_for_inference, target_column, continuous_columns, forecast_horizon)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
