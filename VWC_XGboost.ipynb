{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def trim_start_end_nans(df):\n",
    "    \"\"\"\n",
    "    Removes rows at the start and end of a DataFrame that have NaN values in any column.\n",
    "    \"\"\"\n",
    "    # Initialize start_idx and end_idx based on the DataFrame's index type\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        start_idx = df.index[0]  # Assume first index is earliest; adjust if necessary\n",
    "        end_idx = df.index[-1]  # Assume last index is latest; adjust if necessary\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = len(df) - 1\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Find the first non-NaN index in the current column\n",
    "        first_valid_index = df[column].first_valid_index()\n",
    "        if first_valid_index is not None and df.index.get_loc(\n",
    "            first_valid_index\n",
    "        ) > df.index.get_loc(start_idx):\n",
    "            start_idx = first_valid_index\n",
    "\n",
    "        # Find the last non-NaN index in the current column\n",
    "        last_valid_index = df[column].last_valid_index()\n",
    "        if last_valid_index is not None and df.index.get_loc(\n",
    "            last_valid_index\n",
    "        ) < df.index.get_loc(end_idx):\n",
    "            end_idx = last_valid_index\n",
    "\n",
    "    # Trim the DataFrame\n",
    "    return df.loc[start_idx:end_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_data_for_plot(\n",
    "    plot_number,\n",
    "    target_columns,\n",
    "    continuous_columns,\n",
    "    start_date=\"2023-07-20\",\n",
    "    end_date=\"2023-09-03\",\n",
    "    rolling_windows=[3, 7], \n",
    "):\n",
    "    \"\"\"\n",
    "    Process data for a given plot number within a specified date range. This includes:\n",
    "    * Spike Detection (up and down) for VWC columns\n",
    "    * Time since last significant precipitation\n",
    "    * Cumulative precipitation within a time window\n",
    "    * Rolling window statistics\n",
    "    * Time Encoding\n",
    "    \"\"\"\n",
    "\n",
    "    # Database connection\n",
    "    conn = sqlite3.connect(\"processed_data.db\")\n",
    "    query = \"SELECT * FROM data_table\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Filter by plot_number and date range\n",
    "    df = df[\n",
    "        (df[\"plot_number\"] == plot_number)\n",
    "        & (df[\"TIMESTAMP\"] >= start_date)\n",
    "        & (df[\"TIMESTAMP\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Convert TIMESTAMP to datetime\n",
    "    df[\"TIMESTAMP\"] = pd.to_datetime(df[\"TIMESTAMP\"])\n",
    "    df.set_index(\"TIMESTAMP\", inplace=True)\n",
    "\n",
    "    # Sort by TIMESTAMP \n",
    "    df.sort_values(by=\"TIMESTAMP\", inplace=True)\n",
    "\n",
    "    # Select relevant columns\n",
    "    df = df[continuous_columns + target_columns]\n",
    "\n",
    "    # Resample to daily frequency \n",
    "    df = df.resample(\"D\").mean()\n",
    "\n",
    "    # Spike detection for VWC columns\n",
    "    for col in df.columns:\n",
    "        if \"VWC\" in col:\n",
    "            df[f\"{col}_spike_up\"] = (df[col] > df[col].shift(1) * 1.15).astype(int)  # 15% increase\n",
    "            df[f\"{col}_spike_down\"] = (df[col] < df[col].shift(1) * 0.85).astype(int)  # 15% decrease\n",
    "\n",
    "\n",
    "    # Time since precipitation (modify thresholds as needed)\n",
    "    significant_precip_threshold = 0.5  \n",
    "    max_precip_value = df['precip_irrig'].max()\n",
    "    df['time_since_last_significant_precip'] = (df['precip_irrig'] > significant_precip_threshold).astype(int)\n",
    "    df['time_since_last_significant_precip'] = df['time_since_last_significant_precip'].ffill()\n",
    "    df['time_since_last_half_max_precip'] = (df['precip_irrig'] > (max_precip_value / 2)).astype(int)\n",
    "    df['time_since_last_half_max_precip'] = df['time_since_last_half_max_precip'].ffill()\n",
    "    \n",
    "\n",
    "    # Cumulative precipitation (replace 4 with the desired window)\n",
    "    df['precip_irrig_cumulative_4day'] = df['precip_irrig'].rolling(4).sum() \n",
    "\n",
    "    # Preprocessing \n",
    "    df = df.interpolate(method=\"pchip\")\n",
    "\n",
    "    # Rolling window features\n",
    "    for window in rolling_windows:\n",
    "        for col in continuous_columns:\n",
    "            df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "            df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window).std()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(df, target_columns, continuous_columns):\n",
    "    # Subtract mean from each column (append new columns with suffix \"_mean_subtracted\")\n",
    "    df_mean_subtracted = df.copy()\n",
    "    mean_values = {}\n",
    "    for col in df_mean_subtracted.columns:\n",
    "        if col in [target_columns + continuous_columns]:\n",
    "            mean_values[col] = df_mean_subtracted[col].mean()\n",
    "            df_mean_subtracted[col] = df_mean_subtracted[col] - mean_values[col]\n",
    "    return df_mean_subtracted, mean_values\n",
    "\n",
    "def create_derivative_columns(df, target_columns, continuous_columns):\n",
    "    initial_values = {}\n",
    "    for col in df.columns:  # Change to apply to all columns\n",
    "        if col in [target_columns + continuous_columns]:\n",
    "            initial_values[col] = df[col].iloc[0]\n",
    "        deriv_col_name = f\"{col}_deriv\" \n",
    "        df[deriv_col_name] = df[col].diff().fillna(0)  # Fill NaN with 0 for initial diff\n",
    "    return df, initial_values\n",
    "\n",
    "\n",
    "def transform_and_scale_data(df, target_columns, continuous_columns):\n",
    "    df_transformed = df.copy()\n",
    "    df_transformed, mean_values = subtract_mean(df_transformed, target_columns, continuous_columns)  # Change here to apply to all\n",
    "    df_transformed, initial_values = create_derivative_columns(df_transformed, target_columns, continuous_columns)\n",
    "    df_transformed[\"precip_irrig_bool\"] = df_transformed[\"precip_irrig\"].apply(\n",
    "        lambda x: 1 if x > 0 else 0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def train_and_save_model(X, y, model_save_path):\n",
    "    \"\"\"\n",
    "    Train a model and save it to the specified path.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    best_model = None\n",
    "    best_rmse = float(\"inf\")\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        param = {\n",
    "            'max_depth': 5,\n",
    "            'eta': 0.05,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 1,\n",
    "            'lambda': 1,\n",
    "            'alpha': 0.2,\n",
    "            'gamma': 0.2 \n",
    "        }\n",
    "        num_round = 20\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=param,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=num_round,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=200,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        y_val_pred = bst.predict(dval)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        \n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_model = bst\n",
    "\n",
    "    model_filename = os.path.join(model_save_path, \"next_day_model.pkl\")\n",
    "    with open(model_filename, 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "\n",
    "    return model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for VWC_06 trained and saved at c:\\Users\\bnsoh2\\OneDrive - University of Nebraska-Lincoln\\Projects\\Students\\Bryan Nsoh\\Indep_study_NsohGuo_2024\\ML\\models\\plot_2007\\VWC_06\\next_day_model.pkl for plot 2007\n",
      "Model for VWC_18 trained and saved at c:\\Users\\bnsoh2\\OneDrive - University of Nebraska-Lincoln\\Projects\\Students\\Bryan Nsoh\\Indep_study_NsohGuo_2024\\ML\\models\\plot_2007\\VWC_18\\next_day_model.pkl for plot 2007\n",
      "Model for VWC_30 trained and saved at c:\\Users\\bnsoh2\\OneDrive - University of Nebraska-Lincoln\\Projects\\Students\\Bryan Nsoh\\Indep_study_NsohGuo_2024\\ML\\models\\plot_2007\\VWC_30\\next_day_model.pkl for plot 2007\n",
      "Model for VWC_06 trained and saved at c:\\Users\\bnsoh2\\OneDrive - University of Nebraska-Lincoln\\Projects\\Students\\Bryan Nsoh\\Indep_study_NsohGuo_2024\\ML\\models\\plot_2014\\VWC_06\\next_day_model.pkl for plot 2014\n",
      "Model for VWC_18 trained and saved at c:\\Users\\bnsoh2\\OneDrive - University of Nebraska-Lincoln\\Projects\\Students\\Bryan Nsoh\\Indep_study_NsohGuo_2024\\ML\\models\\plot_2014\\VWC_18\\next_day_model.pkl for plot 2014\n",
      "Model for VWC_30 trained and saved at c:\\Users\\bnsoh2\\OneDrive - University of Nebraska-Lincoln\\Projects\\Students\\Bryan Nsoh\\Indep_study_NsohGuo_2024\\ML\\models\\plot_2014\\VWC_30\\next_day_model.pkl for plot 2014\n",
      "[12.573908  12.472343  12.582901  12.419781  12.410788  12.410788\n",
      " 12.410788  12.674534  13.64875   14.069988  14.069988  13.968423\n",
      " 13.968423  13.968423  13.968423  13.968423  13.776302  14.069988\n",
      " 14.069988  13.7467575 12.964661  12.508795  13.3524475 13.583637\n",
      " 13.583637  13.7467575 13.7467575 13.64875   13.64875   12.805098\n",
      " 12.968218  12.968218  13.64875   13.547186  13.547186  13.645193\n",
      " 13.482073  13.482073  13.384066 ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 145\u001b[0m\n\u001b[0;32m    143\u001b[0m plot_number_for_inference \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2001\u001b[39m  \u001b[38;5;66;03m# Example plot number for inference\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m all_averaged_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_number_for_inference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_plot_numbers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_averaged_predictions)  \u001b[38;5;66;03m# Example output\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 117\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(models_path, plot_number, target_columns, continuous_columns, trained_plot_numbers, forecast_horizon)\u001b[0m\n\u001b[0;32m    115\u001b[0m     approximations \u001b[38;5;241m=\u001b[39m linear_approximation(y_actual[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], averaged_prediction[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], forecast_horizon)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Assign the prediction to the next day and approximations to the subsequent days\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_actual\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m averaged_prediction\n\u001b[0;32m    118\u001b[0m     predictions[\u001b[38;5;28mlen\u001b[39m(y_actual) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m approximations[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the first day since it's the actual prediction\u001b[39;00m\n\u001b[0;32m    120\u001b[0m all_predictions[target_column] \u001b[38;5;241m=\u001b[39m predictions\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the necessary functions are defined as provided:  \n",
    "# trim_start_end_nans, process_data_for_plot, subtract_mean, create_derivative_columns, transform_and_scale_data\n",
    "\n",
    "def prepare_and_train_models(plot_numbers, target_columns, continuous_columns, base_model_save_path):\n",
    "    \"\"\"\n",
    "    Prepare data, train models to predict next day for each target variable, and save the models.\n",
    "    \"\"\"\n",
    "    for plot_number in plot_numbers:\n",
    "        plot_path = os.path.join(base_model_save_path, f\"plot_{plot_number}\")\n",
    "        os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "        for target_column in target_columns:\n",
    "            # Adjust continuous columns to exclude the current target column\n",
    "            adjusted_continuous_columns = [col for col in continuous_columns if col != target_column]\n",
    "\n",
    "            # Process and transform data\n",
    "            df = process_data_for_plot(plot_number, target_columns, adjusted_continuous_columns)\n",
    "            df = trim_start_end_nans(df)\n",
    "            df_transformed = transform_and_scale_data(df, target_columns, adjusted_continuous_columns)\n",
    "\n",
    "            # Define training data\n",
    "            X = df_transformed.drop(columns=target_columns).values\n",
    "            y = df_transformed[target_column].values\n",
    "\n",
    "            # Set up model save directory\n",
    "            model_save_path = os.path.join(plot_path, target_column)\n",
    "            os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "            # Train and save the model for predicting the next day\n",
    "            model_filename = train_and_save_model(X, y, model_save_path)\n",
    "            print(f\"Model for {target_column} trained and saved at {model_filename} for plot {plot_number}\")\n",
    "\n",
    "def predict_with_model(model_path, X):\n",
    "    \"\"\"\n",
    "    Load a model from a file and make predictions.\n",
    "    \"\"\"\n",
    "    with open(model_path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    dtest = xgb.DMatrix(X)\n",
    "    return model.predict(dtest)\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def linear_approximation(current_actual, prediction, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Create a linear approximation for the forecast_horizon based on the last actual value,\n",
    "    the current actual value, and the predicted value.\n",
    "    \"\"\"\n",
    "    # Calculate the slope using the last actual and the prediction\n",
    "    slope = prediction - current_actual\n",
    "\n",
    "    # Start the approximations list with the current actual and prediction\n",
    "    approximations = [current_actual, prediction]\n",
    "\n",
    "    # Extend the line for the next points\n",
    "    for _ in range(2, forecast_horizon):\n",
    "        next_value = approximations[-1] + slope\n",
    "        approximations.append(next_value)\n",
    "\n",
    "    return approximations\n",
    "\n",
    "def plot_predictions_vs_actuals(y_actual, predictions, target_column, plot_number):\n",
    "    \"\"\"\n",
    "    Plot the actual vs predicted values for a given target column.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(y_actual, label=f'Actual {target_column}', linestyle='-', marker='o')\n",
    "    plt.plot(predictions, label=f'Predicted {target_column}', linestyle='--', marker='x')\n",
    "    plt.title(f'Predictions vs Actual Values for {target_column} in Plot {plot_number}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_inference(models_path, plot_number, target_columns, continuous_columns, trained_plot_numbers, forecast_horizon=4):\n",
    "    # Process and transform data for inference\n",
    "    df = process_data_for_plot(plot_number, target_columns, continuous_columns)\n",
    "    df = trim_start_end_nans(df)\n",
    "    df_transformed = transform_and_scale_data(df, target_columns, continuous_columns)\n",
    "\n",
    "    X = df_transformed.drop(columns=target_columns).values\n",
    "    \n",
    "    all_predictions = {}\n",
    "    \n",
    "    for target_column in target_columns:\n",
    "        y_actual = df_transformed[target_column].values\n",
    "        # Initialize predictions array with nan to accommodate actuals plus forecast horizon\n",
    "        predictions = np.full(len(y_actual) + forecast_horizon - 1, np.nan)\n",
    "\n",
    "        # Load model and make predictions for the target column\n",
    "        model_predictions = []\n",
    "        for trained_plot_number in trained_plot_numbers:\n",
    "            model_path = os.path.join(models_path, f\"plot_{trained_plot_number}\", target_column, \"next_day_model.pkl\")\n",
    "            if os.path.exists(model_path):\n",
    "                pred = predict_with_model(model_path, X)\n",
    "                model_predictions.append(pred)\n",
    "            else:\n",
    "                print(f\"Model file not found: {model_path}\")\n",
    "                continue\n",
    "\n",
    "        # Average predictions from all models and make linear approximation for the stride\n",
    "        if model_predictions:\n",
    "            averaged_prediction = np.mean(model_predictions, axis=0)\n",
    "            \n",
    "            #print shape of averaged_prediction\n",
    "            print(averaged_prediction)\n",
    "            # Generate linear approximations for the forecast horizon\n",
    "            approximations = linear_approximation(y_actual[-1], averaged_prediction[-1], forecast_horizon)\n",
    "            # Assign the prediction to the next day and approximations to the subsequent days\n",
    "            predictions[len(y_actual)] = averaged_prediction\n",
    "            predictions[len(y_actual) + 1:] = approximations[1:]  # Skip the first day since it's the actual prediction\n",
    "\n",
    "        all_predictions[target_column] = predictions\n",
    "\n",
    "        # Plot the actual and predicted values\n",
    "        plot_predictions_vs_actuals(y_actual, predictions, target_column, plot_number)\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "# Example usage\n",
    "trained_plot_numbers = [2007, 2014]  # Example plot numbers for training\n",
    "target_columns = [\"VWC_06\", \"VWC_18\", \"VWC_30\"]\n",
    "continuous_columns = [\n",
    "    \"Ta_2m_Avg\", \"RH_2m_Avg\", \"Solar_2m_Avg\", \"WndAveSpd_3m\", \"Rain_1m_Tot\",\n",
    "    \"Dp_2m_Avg\", \"TaMax_2m\", \"TaMin_2m\", \"RHMax_2m\", \"RHMin_2m\",\n",
    "    \"HeatIndex_2m_Avg\", \"irrigation\", \"precip_irrig\", \"canopy_temp\"\n",
    "]\n",
    "\n",
    "model_save_path = os.path.join(os.getcwd(), \"models\")\n",
    "\n",
    "# Train models for each plot number and target variable\n",
    "for plot_number in trained_plot_numbers:\n",
    "    prepare_and_train_models([plot_number], target_columns, continuous_columns, model_save_path)\n",
    "\n",
    "# Inference example\n",
    "plot_number_for_inference = 2001  # Example plot number for inference\n",
    "# Run inference\n",
    "all_averaged_predictions = run_inference(model_save_path, plot_number_for_inference, target_columns, continuous_columns, trained_plot_numbers)\n",
    "print(all_averaged_predictions)  # Example output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
