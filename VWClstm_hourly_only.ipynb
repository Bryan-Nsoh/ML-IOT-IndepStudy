{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Predicting Volumetric Water Content (VWC)\n",
    "\n",
    "This Jupyter notebook focuses on developing and training an LSTM model to predict volumetric water content (VWC) in the soil, a crucial indicator of soil moisture and plant stress, as part of a larger project on automated irrigation scheduling.\n",
    "\n",
    "**Specifics:**\n",
    "\n",
    "* **Model Development and Training:** The notebook builds, trains, and evaluates an LSTM model for VWC prediction, including functions for data preprocessing, sequence creation, model configuration, training, and validation.\n",
    "* **Data Preprocessing and Feature Engineering:** It explores techniques like mean subtraction, derivative calculation, and log transformation to improve model performance by normalizing data, highlighting irrigation/precipitation events, and addressing data distribution issues.\n",
    "* **Performance Evaluation and Challenges:** Time series cross-validation is used to assess modelgeneralizability, identifying challenges such as potential overfitting and difficulties in predicting mean VWC values.\n",
    "* **Future Strategies:** The notebook suggests strategies to address these challenges, including expanding window cross-validation, introducing categorical features, and adjusting filter smoothing parameters.\n",
    "\n",
    "**Contribution to the Project:**\n",
    "\n",
    "This notebook serves as a crucial component of the larger automated irrigation scheduling project by:\n",
    "\n",
    "* Providing a framework for developing and refining the machine learning model for VWC prediction.\n",
    "* Identifying data preprocessing and feature engineering techniques that can improve model performance.\n",
    "* Highlighting areas for further improvement and suggesting potential solutions.\n",
    "\n",
    "The insights and results from this notebook will be used to:\n",
    "\n",
    "* Build a more robust andgeneralizable model for predicting plant stress.\n",
    "* Develop an algorithm to infer the optimal irrigation amount based on predicted VWC.\n",
    "* Integrate the model with the LoRaWAN IoT platform for real-time data collection and irrigation control.\n",
    "\n",
    "In summary, this notebook plays a vital role in developing and refining the machine learning component of the automated irrigation scheduling system, contributing to sustainable and efficient agricultural practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Data Processing Functions\n",
    "\n",
    "This cell defines functions for data preparation:\n",
    "\n",
    "* **`trim_start_end_nans(df)`:** \n",
    "    * Removes rows with NaN values at the start and end of the DataFrame.\n",
    "    * Iterates through each column and identifies the first and last valid indices.\n",
    "    * Returns a trimmed DataFrame.\n",
    "\n",
    "* **`process_data_for_plot(...)`:**\n",
    "    * Processes data for a specific plot within a defined date range.\n",
    "    * Key steps:\n",
    "        * Connects to the database and loads data.\n",
    "        * Filters data by plot number, date range, and relevant columns.\n",
    "        * Encodes timestamps cyclically.\n",
    "        * Eliminates duplicate indices and sets timestamp as index.\n",
    "        * Handles missing values (trims NaNs and interpolates).\n",
    "        * Smooths data in target columns using Savitzky-Golay filter.\n",
    "        * Applies log transformation and creates binary column.\n",
    "        * Returns processed DataFrame.\n",
    "\n",
    "* **`transform_and_scale_data(...)`:**\n",
    "    * Transforms and scales data for neural network input.\n",
    "    * Key steps:\n",
    "        * Copies DataFrame to avoid modifying original.\n",
    "        * Subtracts mean from target columns.\n",
    "        * Calculates difference-based derivatives for target columns.\n",
    "        * Scales all columns using MinMaxScaler with a buffer.\n",
    "        * Creates binary column.\n",
    "        * Returns transformed and scaled DataFrame, updated scalers, and transformation metadata.\n",
    "\n",
    "* **`reverse_transform_and_scale_data(...)`:**\n",
    "    * Reverses transformations and scaling applied to the DataFrame.\n",
    "    * Uses stored metadata to undo scaling, add back mean, and reconstruct original values from derivatives.\n",
    "\n",
    "**Areas of Emphasis:**\n",
    "\n",
    "* **Handling NaN values:** The code uses a combination of trimming and interpolation to handle missing data.\n",
    "* **Data transformations:** The code applies Savitzky-Golay filter and log transformation to specific columns.\n",
    "* **Scaling:** MinMaxScaler is used with a buffer to account for potential future data variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def trim_start_end_nans(df):\n",
    "    \"\"\"\n",
    "    Removes rows at the start and end of a DataFrame that have NaN values in any column.\n",
    "    \"\"\"\n",
    "    # Initialize start_idx and end_idx based on the DataFrame's index type\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        start_idx = df.index[0]  # Assume first index is earliest; adjust if necessary\n",
    "        end_idx = df.index[-1]  # Assume last index is latest; adjust if necessary\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = len(df) - 1\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Find the first non-NaN index in the current column\n",
    "        first_valid_index = df[column].first_valid_index()\n",
    "        if first_valid_index is not None and df.index.get_loc(first_valid_index) > df.index.get_loc(start_idx):\n",
    "            start_idx = first_valid_index\n",
    "\n",
    "        # Find the last non-NaN index in the current column\n",
    "        last_valid_index = df[column].last_valid_index()\n",
    "        if last_valid_index is not None and df.index.get_loc(last_valid_index) < df.index.get_loc(end_idx):\n",
    "            end_idx = last_valid_index\n",
    "\n",
    "    # Trim the DataFrame\n",
    "    return df.loc[start_idx:end_idx]\n",
    "\n",
    "# Now, apply the updated function to the DataFrame\n",
    "\n",
    "def process_data_for_plot(plot_number, target_columns, continuous_columns, start_date='2023-07-20', end_date='2023-09-03'):\n",
    "    \"\"\"\n",
    "    Process data for a given plot number within a specified date range.\n",
    "\n",
    "    Parameters:\n",
    "    - plot_number: The plot number to filter the data by.\n",
    "    - start_date: The start date of the period to filter the data.\n",
    "    - end_date: The end date of the period to filter the data.\n",
    "\n",
    "    Returns:\n",
    "    - A processed DataFrame with the data for the specified plot and date range.\n",
    "    \"\"\"\n",
    "    # Connect to the database and load data\n",
    "    conn = sqlite3.connect('processed_data.db')\n",
    "    query = 'SELECT * from data_table'\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Filter by plot_number\n",
    "    df = df[df['plot_number'] == plot_number]\n",
    "    \n",
    "    # FILTER BY COLUMNS\n",
    "    df = df[['TIMESTAMP'] + target_columns + continuous_columns]    \n",
    "\n",
    "    # Filter by date range\n",
    "    df = df[(df['TIMESTAMP'] >= start_date) & (df['TIMESTAMP'] <= end_date)]\n",
    "\n",
    "    # Timestamp cyclical encoding\n",
    "    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "    df['day_sin'] = np.sin(df['TIMESTAMP'].dt.dayofyear / 365 * 2 * np.pi)\n",
    "    df['day_cos'] = np.cos(df['TIMESTAMP'].dt.dayofyear / 365 * 2 * np.pi)\n",
    "    df['hour_sin'] = np.sin(df['TIMESTAMP'].dt.hour / 24 * 2 * np.pi)\n",
    "    df['hour_cos'] = np.cos(df['TIMESTAMP'].dt.hour / 24 * 2 * np.pi)\n",
    "    df['dow_sin'] = np.sin(df['TIMESTAMP'].dt.dayofweek / 7 * 2 * np.pi)\n",
    "    df['dow_cos'] = np.cos(df['TIMESTAMP'].dt.dayofweek / 7 * 2 * np.pi)\n",
    "\n",
    "\n",
    "    # Eliminate duplicate indices and set TIMESTAMP as index\n",
    "    df = df.drop_duplicates().set_index('TIMESTAMP').sort_index()\n",
    "\n",
    "    # Assuming trim_start_end_nans is a predefined function to handle NaN values\n",
    "    df = trim_start_end_nans(df)\n",
    "\n",
    "    # Interpolate missing values\n",
    "    df = df[target_columns + continuous_columns].interpolate(method='pchip')\n",
    "    \n",
    "    # run savgol filter with no differentiation an window = 10 to target columns\n",
    "    target_columns = ['VWC_06', 'VWC_18', 'VWC_30']\n",
    "    for column in target_columns:\n",
    "        df[column] = savgol_filter(x=df[column], window_length=20, polyorder=4, deriv=0)\n",
    "        \n",
    "    # testing different transforms\n",
    "    df['precip_irrig_log'] = np.log(df['precip_irrig'] + 1)\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "def subtract_mean(df, target_columns):\n",
    "    mean_values = {}\n",
    "    for col in target_columns:\n",
    "        mean_value = df[col].mean()  \n",
    "        df[col] -= mean_value\n",
    "        mean_values[col] = mean_value\n",
    "    return df, mean_values\n",
    "\n",
    "\n",
    "\n",
    "def create_derivative_columns(df, target_columns, keep_original_cols=True):\n",
    "    \"\"\"Calculates difference-based derivatives of specified columns.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame containing the data.\n",
    "        target_columns: List of columns for which derivatives are calculated.\n",
    "        keep_original_cols: If True, creates new columns; otherwise overwrites.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with derivative columns, plus initial values if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    initial_values = {}  # Store initial values for reconstruction\n",
    "\n",
    "    for col in target_columns:\n",
    "        deriv_col_name = f\"{col}_deriv\"\n",
    "\n",
    "        if keep_original_cols:\n",
    "            df[deriv_col_name] = df[col].diff()  # Calculate differences\n",
    "            \n",
    "            #  #Plot the original and derived columns\n",
    "            # plt.figure(figsize=(10, 6))\n",
    "            # plt.plot(df[col], label=col)\n",
    "            # plt.plot(df[deriv_col_name], label=deriv_col_name)\n",
    "            # plt.xlabel('Time')\n",
    "            # plt.ylabel('Value')\n",
    "            # plt.title(f'{col} vs {deriv_col_name}')\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "            \n",
    "        else:\n",
    "            initial_values[col] = df[col].iloc[0]  # Store initial value\n",
    "            df[col] = df[col].diff()  \n",
    "            \n",
    "\n",
    "    return df, initial_values  # Return initial values as well\n",
    "\n",
    "def scale_dataframe(df, scalers):\n",
    "    \"\"\"\n",
    "    Optimally scale all columns in a DataFrame using MinMaxScaler, adjusting for an expanded range with a buffer. \n",
    "    This involves creating a dummy range for scaler fitting, then scaling the original data with the adjusted scaler.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with columns to be scaled.\n",
    "        scalers (dict): Dictionary storing the scalers for each column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Scaled DataFrame using the optimally adjusted scalers.\n",
    "        dict: Dictionary of the adjusted scalers.\n",
    "    \"\"\"\n",
    "    scaled_df = pd.DataFrame()  # Initialize an empty DataFrame for scaled values\n",
    "    for column in df.columns:\n",
    "        # Check if a scaler already exists; if not, proceed to create and fit a new one\n",
    "        if column not in scalers:\n",
    "            # Calculate the original range and apply a 30% buffer\n",
    "            col_min, col_max = df[column].min(), df[column].max()\n",
    "            range_buffer = (col_max - col_min) * 0.3  # 30% buffer\n",
    "            buffered_min = col_min - range_buffer\n",
    "            buffered_max = col_max + range_buffer\n",
    "\n",
    "            # Create a new scaler and fit it on the buffered range\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler.fit([[buffered_min], [buffered_max]])\n",
    "            scalers[column] = scaler\n",
    "\n",
    "        # Scale the original data with the adjusted scaler\n",
    "        scaled_values = scalers[column].transform(df[[column]].values.reshape(-1, 1)).flatten()  # Flatten the array here\n",
    "        scaled_df[column] = scaled_values\n",
    "\n",
    "\n",
    "    return scaled_df, scalers\n",
    "\n",
    "\n",
    "def transform_and_scale_data(df, target_columns, scalers, keep_original_cols=True):\n",
    "    \"\"\"\n",
    "    Transforms and scales the data in the DataFrame for neural network input, returning the\n",
    "    transformed DataFrame, updated scalers, and transformation metadata.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        target_columns (list): List of target column names.\n",
    "        scalers (dict): Dictionary containing MinMaxScalers for each column, can be empty.\n",
    "        keep_original_cols (bool): If True, keeps original columns unchanged except for scaling;\n",
    "                                   if False, applies transformations directly on the target columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed and scaled DataFrame.\n",
    "        dict: Updated dictionary containing MinMaxScalers for each column.\n",
    "        dict: A dictionary containing transformation metadata for the target columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy DataFrame to avoid modifying the original\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    \n",
    "    # Subtract mean from target columns\n",
    "    df_transformed, mean_values = subtract_mean(df_transformed, target_columns)\n",
    "    \n",
    "    # Apply transformations directly using helper functions\n",
    "    df_transformed, initial_values = create_derivative_columns(df_transformed, target_columns, keep_original_cols)\n",
    "    \n",
    "    # Deleting or adjusting rows as necessary (e.g., due to NaN values from differentiation)\n",
    "    df_transformed = df_transformed.iloc[1:]\n",
    "    \n",
    "    # Scale all columns in df_transformed\n",
    "    df_transformed, scalers = scale_dataframe(df_transformed, scalers)\n",
    "    \n",
    "    #print(f\"Shape of transformed DataFrame: {df_transformed.shape}\")\n",
    "    \n",
    "    # Metadata for reverse transformation\n",
    "    transformation_metadata = {\n",
    "        'means': mean_values,\n",
    "        'scalers': scalers,\n",
    "        'keep_original_cols': keep_original_cols,\n",
    "        'initial_values': initial_values\n",
    "    }\n",
    "    \n",
    "    # add 'precip_irrig_bool' as a binned column with 0 or 1\n",
    "    df_transformed['precip_irrig_bool'] = df_transformed['precip_irrig_log'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # Metadata for reverse transformation\n",
    "    transformation_metadata = {\n",
    "        'means': mean_values,\n",
    "        'scalers': scalers,\n",
    "        'keep_original_cols': keep_original_cols,\n",
    "        'initial_values': initial_values,\n",
    "        'columns': df.columns.tolist()\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "    return df_transformed, transformation_metadata\n",
    "\n",
    "\n",
    "\n",
    "def reverse_transform_and_scale_data(df, transformation_metadata, target_columns):\n",
    "    \"\"\"\n",
    "    Reverses the transformations and scaling applied to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The transformed and scaled DataFrame.\n",
    "        transformation_metadata (dict): A dictionary containing transformation metadata.\n",
    "        target_columns (list): List of target columns to be reverse transformed and scaled.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with reversed transformations and scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract metadata\n",
    "    scalers = transformation_metadata['scalers']\n",
    "    mean_values = transformation_metadata['means']\n",
    "    initial_values = transformation_metadata['initial_values']\n",
    "    keep_original_cols = transformation_metadata['keep_original_cols']\n",
    "\n",
    "    # #print dimensions of df\n",
    "    #print(f\"Shape of input DataFrame: {df.shape}\")\n",
    "\n",
    "    # Reverse scale the target columns\n",
    "    for column in target_columns:\n",
    "        if column in scalers:\n",
    "            # Reshape data for inverse_transform\n",
    "            scaled_data = df[[column]].values\n",
    "            # Apply inverse_transform\n",
    "            df[column] = scalers[column].inverse_transform(scaled_data)\n",
    "\n",
    "    # Apply undifferencing with cumsum() \n",
    "    for column in target_columns:\n",
    "        # Reverse subtract mean from target columns if they were mean-adjusted\n",
    "        if column in mean_values:\n",
    "            #print(f\"Mean for {column}: \", mean_values[column])\n",
    "            df[column] = df[column] + mean_values[column]\n",
    "            #print(f\"Reversed mean for {column}: \", df[column].head())\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Model Building and Training Functions\n",
    "\n",
    "This cell defines functions for model building, training, and validation:\n",
    "\n",
    "* **`create_sequences(...)`:**\n",
    "    * Creates sequences of data for training the LSTM model using a sliding window approach.\n",
    "    * Takes input and target data, window size, forecast horizon, and stride as parameters.\n",
    "    * Returns sequences as NumPy arrays.\n",
    "\n",
    "* **`build_model(...)`:**\n",
    "    * Builds the LSTM model architecture with LSTM layers, BatchNormalization, Dropout, and a Dense output layer.\n",
    "    * Compiles the model with Adam optimizer and MSE loss function.\n",
    "    * Returns the compiled model.\n",
    "\n",
    "* **`chronological_split(...)`:**\n",
    "    * Splits data chronologically into training and validation sets based on the provided test_size proportion.\n",
    "\n",
    "* **`scheduler(...)`:**\n",
    "    * Defines a learning rate scheduler that reduces the learning rate exponentially after a certain number of epochs.\n",
    "\n",
    "* **`train_step(...)`:**\n",
    "    * Defines a single training step for the model using GradientTape to calculate gradients and update weights.\n",
    "    * Updates learning rate based on the scheduler.\n",
    "\n",
    "* **`val_step(...)`:**\n",
    "    * Defines a single validation step, calculating the loss on validation data without updating weights.\n",
    "\n",
    "* **`run_workflow(...)`:**\n",
    "    * Orchestrates the training and validation process:\n",
    "        * Iterates through each transformed DataFrame.\n",
    "        * Creates sequences for each DataFrame.\n",
    "        * Splits sequences chronologically into training and validation sets using TimeSeriesSplit.\n",
    "        * Builds the LSTM model.\n",
    "        * Trains the model for specified epochs, performing training and validation steps for each fold.\n",
    "        * Implements early stopping and saves the best model.\n",
    "\n",
    "**Areas of Emphasis:**\n",
    "\n",
    "* **Model architecture:** The LSTM model uses multiple layers with decreasing units and regularization techniques.\n",
    "* **Loss function:** MSE is used as the loss function to penalize prediction errors.\n",
    "* **Validation strategy:** TimeSeriesSplit ensures chronological validation, preventing the model from learning from future data.\n",
    "* **Early stopping:** The training process stops early if validation loss doesn't improve for a specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Reshape, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2 \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sequence Creation Function\n",
    "def create_sequences(input_data, target_data, input_window, forecast_horizon, stride=1):\n",
    "    X, y = [], []\n",
    "    for start in range(len(input_data) - input_window - forecast_horizon + stride):\n",
    "        end = start + input_window\n",
    "        X.append(input_data[start:end])\n",
    "        y.append(target_data[end:end + forecast_horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_targets):\n",
    "    model = Sequential([\n",
    "        LSTM(256, input_shape=input_shape, return_sequences=True, kernel_regularizer=l2(0.02)), \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(128, return_sequences=True),  \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),  \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        LSTM(64, return_sequences=True),  \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dense(96 * num_targets), \n",
    "        Reshape((96, num_targets)) \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chronological_split(X, y, test_size=0.2):\n",
    "    # Calculate the index to split the data\n",
    "    total_samples = len(X)\n",
    "    split_index = int(total_samples * (1 - test_size))\n",
    "    \n",
    "    # Split the data chronologically\n",
    "    X_train = X[:split_index]\n",
    "    X_val = X[split_index:]\n",
    "    y_train = y[:split_index]\n",
    "    y_val = y[split_index:]\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Learning Rate Scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 15:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.0001)\n",
    "\n",
    "\n",
    "# Define the training step\n",
    "@tf.function(reduce_retracing=True)\n",
    "def train_step(model, optimizer, loss_function, x_batch, y_batch, epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_batch, training=True)\n",
    "        loss = loss_function(y_batch, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # Update the learning rate correctly using TensorFlow operations\n",
    "    lr = scheduler(epoch, optimizer.learning_rate)\n",
    "    optimizer.learning_rate.assign(lr)\n",
    "    return loss\n",
    "\n",
    "# Define the validation step\n",
    "@tf.function(reduce_retracing=True)\n",
    "def val_step(model, loss_function, x_batch, y_batch):\n",
    "    predictions = model(x_batch, training=False)\n",
    "    loss = loss_function(y_batch, predictions)\n",
    "    return loss\n",
    "\n",
    "def run_workflow(training_data_transformed, target_variables, input_window, forecast_horizon, stride, epochs, batch_size, patience, checkpoint_path):\n",
    "    X_train_all, X_val_all, y_train_all, y_val_all = [], [], [], []\n",
    "    dataframe_identifiers = []\n",
    "    n_splits = 5  # Define the number of splits for cross-validation\n",
    "\n",
    "    # Data preparation\n",
    "    for i, transformed_data in enumerate(training_data_transformed()):\n",
    "        df = transformed_data[0]\n",
    "        X, y = create_sequences(df.values, df[target_variables].values, input_window, forecast_horizon, stride)\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "            #print(f\"Processing fold {fold+1}/{n_splits} for DataFrame_{i}\")\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "            \n",
    "            X_train_all.append(X_train)\n",
    "            X_val_all.append(X_val)\n",
    "            y_train_all.append(y_train)\n",
    "            y_val_all.append(y_val)\n",
    "        \n",
    "        dataframe_identifiers.append(f\"DataFrame_{i}\")\n",
    "\n",
    "    # Model and training setup\n",
    "    model = build_model((input_window, X_train_all[0].shape[2]), len(target_variables))\n",
    "    initial_learning_rate = 0.001 \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "    loss_function = tf.keras.losses.MeanSquaredError()\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    # Training and validation\n",
    "    for epoch in range(epochs):\n",
    "        #print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        epoch_val_losses = []\n",
    "        \n",
    "        # Update learning rate using the scheduler function\n",
    "        new_lr = scheduler(epoch, optimizer.learning_rate.numpy())\n",
    "        optimizer.learning_rate.assign(new_lr)\n",
    "        #print(f\"Current learning rate: {optimizer.learning_rate.numpy()}\")\n",
    "\n",
    "        for fold in range(n_splits):\n",
    "            #print(f\"Training on fold {fold + 1}/{n_splits}\")\n",
    "            X_train, y_train = X_train_all[fold], y_train_all[fold]\n",
    "\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            for x_batch, y_batch in dataset:\n",
    "                train_step(model, optimizer, loss_function, x_batch, y_batch, epoch)\n",
    "\n",
    "            # Validation\n",
    "            X_val, y_val = X_val_all[fold], y_val_all[fold]\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_losses = []\n",
    "            for x_batch, y_batch in val_dataset:\n",
    "                val_loss = val_step(model, loss_function, x_batch, y_batch)\n",
    "                val_losses.append(val_loss.numpy())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            epoch_val_losses.append(avg_val_loss)\n",
    "            #print(f\"Validation loss for fold {fold+1}/{n_splits}: {avg_val_loss}\")\n",
    "\n",
    "        # Average validation loss across folds\n",
    "        average_val_loss = np.mean(epoch_val_losses)\n",
    "        #print(f\"Average validation loss for epoch {epoch+1}: {average_val_loss}\")\n",
    "\n",
    "        # Early stopping and model saving logic\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss\n",
    "            model.save(checkpoint_path)\n",
    "            #print(f\"Model saved at epoch {epoch+1} with validation loss: {average_val_loss}\")\n",
    "            wait = 0  # Reset wait counter after improvement\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                #print(f\"Stopping early due to no improvement in validation loss for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    return checkpoint_path, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Inference and Plotting\n",
    "\n",
    "This cell defines functions for inference and plotting predictions:\n",
    "\n",
    "* **`plot_predictions(...)`:**\n",
    "    * Plots predicted and actual values for target columns of a specific plot.\n",
    "\n",
    "* **`predict_with_sliding_window(...)`:**\n",
    "    * Performs inference on new data using a sliding window approach.\n",
    "    * Predicts target variables for each sequence using the trained model.\n",
    "    * Reverses transformations and scaling.\n",
    "    * Plots predictions and actuals for comparison.\n",
    "\n",
    "**Areas of Emphasis:**\n",
    "\n",
    "* **Sliding window inference:** The model predicts on sequences created by sliding a window across the new data.\n",
    "* **Reverse transformation:** Predictions are transformed back to the original scale and format for interpretation.\n",
    "* **Visualization:** Predictions and actuals are plotted for visual comparison and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './LSTM_dayhour/transformation_metadata2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./LSTM_dayhour\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# open the transformation metadata file\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/transformation_metadata2.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     11\u001b[0m     transformation_metadata \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_predictions\u001b[39m(predictions_df, actuals_df, target_columns, plot_number):\n",
      "File \u001b[1;32mc:\\Users\\bnsoh2\\Desktop\\venvs\\ml_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './LSTM_dayhour/transformation_metadata2.pkl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def plot_predictions(predictions_df, actuals_df, target_columns, plot_number):\n",
    "    num_plots = len(target_columns)\n",
    "    num_rows = int(np.ceil(num_plots / 2))  # You can adjust the number of columns per row if you like\n",
    "    plt.figure(figsize=(15, 5 * num_rows))  # Adjust the figure size as needed\n",
    "    for i, column in enumerate(target_columns):\n",
    "        plt.subplot(num_rows, 2, i + 1)  # Adjust the number of columns per row if you like\n",
    "        plt.plot(predictions_df.index, predictions_df[column], label=f'Predicted {column}', color='red', alpha=0.7)\n",
    "        plt.plot(actuals_df.index, actuals_df[column], label=f'Actual {column}', color='blue')\n",
    "        plt.legend()\n",
    "        plt.title(f'{column} for Plot {plot_number}')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Value')\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def predict_with_sliding_window(model, df, current_transformation_metadata, target_columns, input_window, forecast_horizon, plot_number):\n",
    "    # Create input& target data, input data is all columns except target columns\n",
    "    target_data = df[target_columns].values\n",
    "    input_data = df.values\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    timestamps = []\n",
    "    \n",
    "    # first iteration flag\n",
    "    first_iter = True\n",
    "\n",
    "    # Adjust stride based on prediction: 1 for continuous sliding\n",
    "    stride = forecast_horizon\n",
    "    for start in range(0, len(df) - input_window - forecast_horizon + 1, stride):\n",
    "        end = start + input_window\n",
    "        if end + forecast_horizon > len(input_data):\n",
    "            break\n",
    "        sequence = input_data[start:end]\n",
    "        sequence = sequence.reshape((1, input_window, len(input_data[0])))\n",
    "\n",
    "        \n",
    "\n",
    "        pred = model.predict(sequence)\n",
    "\n",
    "        \n",
    "        if first_iter:\n",
    "            #print(f\"Sequence shape: {sequence.shape}\")\n",
    "            # To #print the top 3 lines, slice the second dimension (sequence length)\n",
    "            #print(\"Sequence (top 3 lines):\")\n",
    "           # #print(sequence[0, :3, :])  # Access the first element of the first dimension, then slice\n",
    "            #print(f\"Predicted shape: {pred.shape}\")\n",
    "            # Similarly, for the predictions, slice the second dimension\n",
    "           #print(\"Predicted (top 3 lines):\")\n",
    "            #print(pred[0, :3, :])  # Access the first element of the first dimension, then slice\n",
    "        \n",
    "            first_iter = False\n",
    "\n",
    "\n",
    "        pred = pred.reshape(-1, pred.shape[-1])\n",
    "        predictions.append(pred)\n",
    "        actuals.append(target_data[end:end + forecast_horizon])\n",
    "        timestamp_series = pd.Series(df.index[end:end + forecast_horizon], index=df.index[end:end + forecast_horizon])\n",
    "        timestamps.append(timestamp_series)\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "    timestamps = pd.concat(timestamps)\n",
    "\n",
    "    # Create the DataFrame with the reshaped predictions and the correct timestamps\n",
    "    predictions_df = pd.DataFrame(predictions, columns=target_columns, index=timestamps)\n",
    "    actuals_df = pd.DataFrame(actuals, columns=target_columns, index=timestamps)\n",
    "\n",
    "    # Apply reverse transformation and scaling to both predictions and actuals\n",
    "\n",
    "    # Plot the predictions and actuals before reverse transformation and scaling\n",
    "    #print(\"Plotting predictions and actuals before reverse transformation and scaling\")\n",
    "    #print(\"Values of predictions_df  and actuals_df before reverse transformation and scaling\")\n",
    "    #print(predictions_df.head(3))\n",
    "    #print(actuals_df.head(3))\n",
    "\n",
    "    plot_predictions(predictions_df, actuals_df, target_columns, plot_number)\n",
    "\n",
    "    # Apply reverse transformation and scaling to both predictions and actuals\n",
    "    predictions_df = reverse_transform_and_scale_data(predictions_df, current_transformation_metadata, target_columns)\n",
    "    actuals_df = reverse_transform_and_scale_data(actuals_df, current_transformation_metadata, target_columns)\n",
    "\n",
    "    # Example usage\n",
    "    #print(\"Values of predictions_df  and actuals_df after reverse transformation and scaling\")\n",
    "    #print(predictions_df.head(3))\n",
    "    #print(actuals_df.head(3))\n",
    "    #print(\"Plotting predictions and actuals after reverse transformation and scaling\")\n",
    "    plot_predictions(predictions_df, actuals_df, target_columns, plot_number)\n",
    "\n",
    "    return predictions_df, actuals_df, timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Training and Inference Workflow\n",
    "\n",
    "This cell demonstrates the complete workflow for training and evaluating an LSTM model on multiple dataframes, as well as performing inference on new data and visualizing the results. \n",
    "\n",
    "**1. Training:**\n",
    "\n",
    "* **Dataframe Selection:** Specify the plot numbers for the dataframes you want to use for training. \n",
    "* **Variable Initialization:** Initialize scalers and define target columns, continuous columns, and training parameters (input window, forecast horizon, epochs, etc.).\n",
    "* **Data Processing:** Use lambda functions or other methods to process and transform the dataframes consistently. \n",
    "* **Model Training:** Run the `run_workflow` function to train the model. This function should handle:\n",
    "    * Data preparation\n",
    "    * Model creation and training\n",
    "    * Saving the best model and transformation metadata\n",
    "\n",
    "**2. Inference:**\n",
    "\n",
    "* **Model Loading:** Load the saved model for inference.\n",
    "* **Data Preparation:** Process and transform the new data using the same methods and transformation metadata as the training data. \n",
    "* **Prediction:** Use the loaded model to make predictions on the transformed new data.\n",
    "* **Reverse Transformation:** Transform the predictions back to the original scale and format using the saved transformation metadata.\n",
    "* **Visualization:** Plot the predictions and actual values for comparison and evaluation. \n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Consistency:** Ensure consistency in data processing and transformation between training and inference.\n",
    "* **Parameter Tuning:** Adjust training parameters to optimize model performance.\n",
    "* **Model and Metadata Saving:** Save the best model and transformation metadata for reliable inference.\n",
    "* **Visualization:** Use plots to assess the quality of predictions and compare them to actual values. \n",
    "\n",
    "By following this workflow, you can effectively train an LSTM model and use it to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on plots [2009, 2004, 2007], testing on plot 2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 16:38:31.455685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-02 16:38:32.003267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30965 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:da:00.0, compute capability: 7.0\n",
      "2024-03-02 16:38:36.850047: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.9.1-py39/lib/python3.9/site-packages/tensorflow/python/../../../../libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LSTM_dayhour/assets\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "#plot_numbers = [2014, 2015, 2007, 2013]\n",
    "#plot_numbers = [2013, 2014, 2015, 2007]\n",
    "#plot_numbers = [2014, 2013, 2015]\n",
    "plot_numbers = [2015, 2013, 2014]\n",
    "plot_numbers = [2009, 2004, 2007, 2003]\n",
    "\n",
    "\n",
    "# Initialize a scaler\n",
    "scalers = {}\n",
    "\n",
    "# *** Important: Update the target_columns and continuous_columns based on your chosen target columns ***\n",
    "# *** Any columns included in target_columns should be removed from continuous_columns ***\n",
    "target_columns = ['VWC_06', 'VWC_18', 'VWC_30']\n",
    "time_columns = ['day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "continuous_columns = [\n",
    "    'Ta_2m_Avg', 'RH_2m_Avg', 'Solar_2m_Avg', 'WndAveSpd_3m', 'Rain_1m_Tot', 'Dp_2m_Avg',\n",
    "    'TaMax_2m', 'TaMin_2m', 'RHMax_2m', 'RHMin_2m', 'HeatIndex_2m_Avg',\n",
    "    'irrigation', 'precip_irrig', 'canopy_temp'\n",
    "]\n",
    "derivative_columns = ['precip_irrig']\n",
    "input_window = 168\n",
    "forecast_horizon = 96\n",
    "stride = 1\n",
    "epochs = 1\n",
    "patience = 150\n",
    "batch_size = 128\n",
    "checkpoint_path = \"./LSTM_dayhour\"\n",
    "\n",
    "# Set to True to keep original columns unchanged\n",
    "keep_original_cols = True\n",
    "\n",
    "\n",
    "\n",
    "def train_and_test_on_all_combinations(plot_numbers, target_columns, continuous_columns, time_columns, derivative_columns,\n",
    "                                  input_window, forecast_horizon, stride, epochs, patience, batch_size, checkpoint_path,\n",
    "                                  keep_original_cols):\n",
    "    for i in range(len(plot_numbers)):\n",
    "        train_plots = plot_numbers[:-1]\n",
    "        test_plot = plot_numbers[-1]\n",
    "\n",
    "        print(f\"Training on plots {train_plots}, testing on plot {test_plot}\")\n",
    "\n",
    "        # Lambda function to handle training data and plot numbers\n",
    "        training_data_dfs = lambda: [process_data_for_plot(plot, target_columns=target_columns, continuous_columns=continuous_columns) for plot in train_plots]\n",
    "\n",
    "        # run once to get the transformation metadata\n",
    "        _, transformation_metadata = transform_and_scale_data(training_data_dfs()[0], target_columns, scalers, keep_original_cols)\n",
    "        \n",
    "        # Store the transformation metadata in checkpoint_path\n",
    "        with open(f\"{checkpoint_path}/transformation_metadata2.pkl\", \"wb\") as file:\n",
    "            pickle.dump(transformation_metadata, file)\n",
    "        \n",
    "        # re-run with the transformation metadata for all training dataframes\n",
    "        training_data_transformed = lambda: [transform_and_scale_data(df, target_columns, transformation_metadata['scalers'], keep_original_cols) for df in training_data_dfs()]\n",
    "\n",
    "        # Train the model\n",
    "        checkpoint_path, model = run_workflow(training_data_transformed, target_columns, \n",
    "                                      input_window, forecast_horizon, stride, epochs, batch_size, patience, checkpoint_path)\n",
    "\n",
    "        # Prepare test data\n",
    "        df_plot = process_data_for_plot(test_plot, target_columns, continuous_columns)\n",
    "\n",
    "        # Drop VWC_42 column if it exists\n",
    "        if 'VWC_42' in df_plot.columns:\n",
    "            df_plot = df_plot.drop(columns=['VWC_42'])\n",
    "\n",
    "        # Transform test data using existing transformation metadata\n",
    "        df_plot, _ = transform_and_scale_data(df_plot, target_columns, transformation_metadata['scalers'], keep_original_cols)\n",
    "\n",
    "        # Make predictions and store results\n",
    "        predictions_df, actuals_df, timestamps = predict_with_sliding_window(\n",
    "            model, df_plot, current_transformation_metadata=transformation_metadata,\n",
    "            target_columns=target_columns, input_window=24*7, forecast_horizon=96, plot_number=test_plot\n",
    "        )\n",
    "\n",
    "\n",
    "# Run the training and testing loop\n",
    "train_and_test_on_all_combinations(plot_numbers, target_columns, continuous_columns, time_columns, derivative_columns,\n",
    "                                  input_window, forecast_horizon, stride, epochs, patience, batch_size, checkpoint_path,\n",
    "                                  keep_original_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
